{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be669824",
   "metadata": {},
   "source": [
    "# NLP encoding and decoding and tokenizers\n",
    "\n",
    "#### Contents\n",
    "0. Install packages\n",
    "1. A basic example of encoding and decoding\n",
    "2. OpenAI's tiktoken\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b60bcc",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a3a8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "678729a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.1.2-cp39-cp39-macosx_10_9_x86_64.whl (728 kB)\n",
      "\u001b[K     |████████████████████████████████| 728 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blobfile>=2\n",
      "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2022.3.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (1.26.9)\n",
      "Requirement already satisfied: filelock~=3.0 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (3.6.0)\n",
      "Collecting pycryptodomex~=3.8\n",
      "  Downloading pycryptodomex-3.16.0-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from blobfile>=2->tiktoken) (4.9.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/michielbontenbal/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Installing collected packages: pycryptodomex, blobfile, tiktoken\n",
      "Successfully installed blobfile-2.0.1 pycryptodomex-3.16.0 tiktoken-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba5477",
   "metadata": {},
   "source": [
    "## 1. A basic example of encoding and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dummy text\n",
    "text = 'the quick brown fox jumps over the lazy dog!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dba53da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#create a set (each character one time) convert it to a list and sort it\n",
    "print(sorted(list(set(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6366b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      " !abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "print(''.join(chars)) #join the items in item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "679ea68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6, 13, 13, 16, 0, 24, 16, 19, 13, 5, 1]\n",
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = str_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = int_to_str = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95dd79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h e l l o\n"
     ]
    }
   ],
   "source": [
    "print(chars[9],chars[6],chars[13], chars[13], chars[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00b2acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape is: torch.Size([44])\n",
      "the dtype is: torch.int64\n",
      "the tensor is: \n",
      "tensor([21,  9,  6,  0, 18, 22, 10,  4, 12,  0,  3, 19, 16, 24, 15,  0,  7, 16,\n",
      "        25,  0, 11, 22, 14, 17, 20,  0, 16, 23,  6, 19,  0, 21,  9,  6,  0, 13,\n",
      "         2, 27, 26,  0,  5, 16,  8,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "# we use PyTorch: https://pytorch.org\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'the shape is: {data.shape}')\n",
    "print(f'the dtype is: {data.dtype}')\n",
    "print(f'the tensor is: \\n{data}') # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b66e9",
   "metadata": {},
   "source": [
    "## 2. Tokenization with tiktoken\n",
    "\n",
    "source: https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a251312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "072b2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'gpt2'>\n",
      "<class 'tiktoken.core.Encoding'>\n"
     ]
    }
   ],
   "source": [
    "print(enc)\n",
    "print(type(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622a599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838e8451",
   "metadata": {},
   "source": [
    "## 3. NLP @ PicNic\n",
    "\n",
    "definition: NLP enables machines to understand human language\n",
    "\n",
    "#### Use cases\n",
    "- Chatbots\n",
    "- Recommending answers to call center agents eg based on sentiment\n",
    "- Identifying trends in customer feedback\n",
    "- Prioritization of tickets\n",
    "- Routing tickets\n",
    "\n",
    "Messages could be: positive feedback from happy customers, request to adjust delivery time, missing products, issues with rproducts eg freshness, payment questions, suggestion to products\n",
    "\n",
    "Automation with NLP.\n",
    "Picnic uses two models:\n",
    "- positive classification model =>assign positivity score => then business rules (eg no question mark, no address etc_ => then close message automatically. Circa 20% of messages are positive\n",
    "- issue classification model => freshness, completeness, payment, adjustment, assortment suggestion\n",
    "\n",
    "Text classification:\n",
    "Step 1: Text cleaning\n",
    "    a. Text substitution: example thursday => $weekday, ? => $QUESTION\n",
    "    b. Replace emojis with words\n",
    "    c. Clean punctuation\n",
    "    d. Tokenization\n",
    "    e. Stemming\n",
    "    f. Remove stop words (eg. are, and, the, etc)\n",
    "Step 2: Feature cration\n",
    "    a. Text as a feature: bag of words, N-grams, TF_IDF term ferquency-inverse document frequency, BERT (embeddings)\n",
    "    \n",
    "    TF-IDF  numbers respresenting how relevant each word is in that document. Overweighr: rare words, underweight: often used words. sklearn has Class TFidfVectorizer\n",
    "    BERT: can do Sentiment analysis, question answering text prediction etc. + get the essence of a text. BERT helps google to understand queries better. \n",
    "    Two submodels:\n",
    "        1. Masked Language Model (MLM) => bidirectional training, 15% of words is masked during training, use words on either siee of the masked word to predict them. Learns context & doesn't require labeled data.    \n",
    "        2. Next sentence prediction. (NSP) Binary classification task. Learn about relationships between sentences. \n",
    "\n",
    "There is a combined loss function of NSP & MLM. \n",
    "\n",
    "BERTje = Dutch BERT model developed at University of Groningen. \n",
    "\n",
    "Huggingface provides really nice API's:\n",
    "from transformers import BertTokeniser, BertforSequenceClassification (oid)\n",
    "\n",
    "TF-IDF works better when other features are present computationally less expensive. BERT is better for unstructured text.\n",
    "\n",
    "Step 3: Classification\n",
    "\n",
    "- logistic regression model\n",
    "    positive score, probabilty of each issue type.\n",
    "    \n",
    "\n",
    "Architecture overview: \n",
    "- Salesforce as system of record\n",
    "- Python job to request all messages (first batch job, now an API)\n",
    "- then 3 AI steps\n",
    "- send back the classifications to sales\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Öykü \n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
